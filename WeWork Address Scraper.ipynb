{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Packages\n",
    "import pycurl\n",
    "from StringIO import StringIO\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pandas.io.json import json_normalize\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the HTML from each WeWork Location\n",
    "url = 'https://www.wework.com/locations'\n",
    "response = requests.get(url, params={\"search_api_views_fulltext\": \"\"})\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "#  note: if this link doesn't work, find the new tag by inspecting one of the city links\n",
    "locations = soup.find_all(\"a\", {\"class\":\"sl_norewrite marketLink__countryList__F4CBD baseLink__countryList__22den weLink-sc-5sbo5g-0 kbLrON\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Clean Location Link Stubs\n",
    "link_stub_list = []\n",
    "for loc in range(0,len(locations)):\n",
    "    selector = locations[loc] \n",
    "    link_stub = selector['href']\n",
    "    link_stub_list.append(link_stub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coworking_spaces = []\n",
    "# Iterating Over Each location\n",
    "for city in tqdm(link_stub_list):\n",
    "    delay = 0\n",
    "    # Trying again if call fails, and therefore space is empty\n",
    "    space = []\n",
    "    while len(space) < 1 :\n",
    "        # Creating dynamic link based on link stub\n",
    "        url = \"https://www.wework.com\" + city\n",
    "        response = requests.get(url, params={\"search_api_views_fulltext\": \"\"})\n",
    "        soup = BeautifulSoup(response.text, \"lxml\")\n",
    "        # Test to see if location is coming soon \n",
    "        coming_soon = soup.find_all(\"div\", {\"class\":\"ray-tag coming-soon_tag\"})\n",
    "        # Break statement to continue on if the location is coming soon\n",
    "        if len(coming_soon) > 1:\n",
    "            print(city + \" Coming Soon, Skipped\")\n",
    "            break\n",
    "        # Getting info from each location (if it doesn't work, inspect inspect the coworking space address update)\n",
    "        space = soup.find_all(\"div\", {\"class\":\"ray-text--body building-card__address\"})\n",
    "        # Adding a delay to avoid timeouts ; this keeps on growing each time the call fails\n",
    "        delay = delay +.1\n",
    "        time.sleep(delay)\n",
    "    # Appending messy location data to a list of jsons\n",
    "    coworking_spaces.append(space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening out the nested list\n",
    "flat_list = [item for sublist in coworking_spaces for item in sublist]\n",
    "# Creating list to put cleaned strings in\n",
    "cleaned_full_address_list = []\n",
    "cleaned_state_list = []\n",
    "cleaned_zip_list = []\n",
    "# Cleaning up addresses to extract the full address, state, and zip\n",
    "for item in range(0,len(flat_list)):\n",
    "    messy_address = flat_list[item]\n",
    "    cleaner_address = str(messy_address).split(\">\")[1]\n",
    "    cleaned_address = cleaner_address.split(\"<\")[0]\n",
    "    state = cleaned_address.split(\" \")[-2] \n",
    "    zipcode = cleaned_address.split(\" \")[-1]\n",
    "    # Trying to avoid adding international locations; this FAILS is a few locations based on string patters that look like USA \n",
    "    if len(zipcode)==5 and len(state)== 2:\n",
    "        cleaned_state_list.append(state)\n",
    "        cleaned_zip_list.append(zipcode)\n",
    "    else:\n",
    "        cleaned_state_list.append(\"\")\n",
    "        cleaned_zip_list.append(\"\")\n",
    "    # Appending the full address to the list\n",
    "    cleaned_full_address_list.append(cleaned_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Lists into a dataframe\n",
    "address_df = pd.DataFrame(list(zip(cleaned_full_address_list, cleaned_state_list, cleaned_zip_list)), columns =['Full Address', 'State', 'Zip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing CSV to local directory\n",
    "address_df.to_csv(\"WeWork Addresses.csv\", encoding = 'utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
